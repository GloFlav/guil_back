"""
üß† LLM EXPLAINER SERVICE V2 - CORRIG√â
G√©n√®re des explications intelligentes pour les r√©sultats ML
CORRECTIONS:
- D√©tection correcte des scores parfaits (100% = CRITIQUE)
- Score de sant√© r√©aliste
- Pas de "pr√™t pour production" si overfitting
- Messages coh√©rents
"""

import logging
from typing import Dict, Any, List, Optional
from datetime import datetime

logger = logging.getLogger(__name__)


class LLMExplainerService:
    """Service de g√©n√©ration d'explications ML intelligentes"""
    
    # Traductions des variables malgaches
    VARIABLE_TRANSLATIONS = {
        "Inona no jiro ampiasainar": "Source d'√©nergie principale",
        "Aiza no maka rano fisotro": "Source d'eau potable",
        "Velaran-tany": "Surface de terrain",
        "Karazana tany": "Type de terrain",
        "Tanim-bary": "Rizi√®re",
        "Faritra": "R√©gion",
        "Fokontany": "Village",
        "Kaominina": "Commune",
    }
    
    def __init__(self):
        self.initialized = True
    
    async def generate_ml_explanation(
        self,
        ml_results: Dict[str, Any],
        data_summary: Dict[str, Any],
        overfitting_detected: bool = False
    ) -> Dict[str, Any]:
        """
        G√©n√®re une explication compl√®te des r√©sultats ML
        
        Args:
            ml_results: R√©sultats du pipeline ML
            data_summary: R√©sum√© des donn√©es (features, samples, etc.)
            overfitting_detected: Flag d'overfitting du pipeline
            
        Returns:
            Dict avec explanation, recommendations, diagnostic, tts_text
        """
        try:
            # Extraire les infos cl√©s
            best_model = ml_results.get('best_model', {})
            model_name = best_model.get('name', 'Mod√®le inconnu')
            problem_type = ml_results.get('problem_type', 'classification')
            test_metrics = ml_results.get('test_metrics', {})
            models_trained = ml_results.get('models_trained', {})
            warnings = ml_results.get('warnings', [])
            
            # Extraire les m√©triques
            train_acc = self._get_train_accuracy(models_trained, model_name)
            val_acc = self._get_val_accuracy(models_trained, model_name)
            test_acc = test_metrics.get('accuracy', test_metrics.get('f1', 0))
            
            # Donn√©es
            n_features = data_summary.get('total_features', 0)
            n_train = data_summary.get('train_size', 1)
            n_test = data_summary.get('test_size', 0)
            ratio = n_features / max(n_train, 1)
            
            # üî¥ D√âTECTION CRITIQUE: Score parfait = probl√®me grave
            # Score parfait si TEST >= 99% (le plus important) OU train ET val >= 99%
            is_perfect_test = test_acc >= 0.99
            is_perfect_train_val = train_acc >= 0.99 and val_acc >= 0.99
            is_perfect_score = is_perfect_test or is_perfect_train_val
            has_data_leakage = is_perfect_test and is_perfect_train_val  # Tout √† 99%+
            
            # Calculer l'√©cart train/val
            train_val_gap = abs(train_acc - val_acc) if train_acc and val_acc else 0
            
            # D√©terminer le niveau de criticit√©
            is_critical = is_perfect_score or has_data_leakage or ratio > 0.3
            is_warning = train_val_gap > 0.15 or ratio > 0.2 or overfitting_detected
            
            # G√©n√©rer les composants
            explanation = self._generate_explanation(
                model_name=model_name,
                problem_type=problem_type,
                train_acc=train_acc,
                val_acc=val_acc,
                test_acc=test_acc,
                n_features=n_features,
                n_train=n_train,
                n_test=n_test,
                ratio=ratio,
                is_perfect_score=is_perfect_score,
                has_data_leakage=has_data_leakage,
                train_val_gap=train_val_gap,
                warnings=warnings
            )
            
            diagnostic = self._generate_diagnostic(
                train_acc=train_acc,
                val_acc=val_acc,
                test_acc=test_acc,
                ratio=ratio,
                n_train=n_train,
                is_perfect_score=is_perfect_score,
                has_data_leakage=has_data_leakage,
                train_val_gap=train_val_gap,
                overfitting_detected=overfitting_detected
            )
            
            recommendations = self._generate_recommendations(
                is_perfect_score=is_perfect_score,
                has_data_leakage=has_data_leakage,
                ratio=ratio,
                train_val_gap=train_val_gap,
                test_acc=test_acc,
                train_acc=train_acc,
                val_acc=val_acc,
                n_train=n_train,
                warnings=warnings
            )
            
            tts_text = self._generate_tts(
                model_name=model_name,
                test_acc=test_acc,
                diagnostic=diagnostic,
                is_perfect_score=is_perfect_score,
                train_acc=train_acc,
                val_acc=val_acc
            )
            
            return {
                "success": True,
                "explanation": explanation,
                "recommendations": recommendations,
                "diagnostic": diagnostic,
                "tts_text": tts_text,
                "generated_at": datetime.now().isoformat()
            }
            
        except Exception as e:
            logger.error(f"Erreur g√©n√©ration explication: {e}")
            return {
                "success": False,
                "error": str(e)
            }
    
    def _get_train_accuracy(self, models_trained: Dict, model_name: str) -> float:
        """R√©cup√®re l'accuracy d'entra√Ænement"""
        if model_name in models_trained:
            train_metrics = models_trained[model_name].get('train_metrics', {})
            return train_metrics.get('accuracy', train_metrics.get('f1', 0))
        return 0
    
    def _get_val_accuracy(self, models_trained: Dict, model_name: str) -> float:
        """R√©cup√®re l'accuracy de validation"""
        if model_name in models_trained:
            val_metrics = models_trained[model_name].get('val_metrics', {})
            return val_metrics.get('accuracy', val_metrics.get('f1', 0))
        return 0
    
    def _generate_explanation(
        self,
        model_name: str,
        problem_type: str,
        train_acc: float,
        val_acc: float,
        test_acc: float,
        n_features: int,
        n_train: int,
        n_test: int,
        ratio: float,
        is_perfect_score: bool,
        has_data_leakage: bool,
        train_val_gap: float,
        warnings: List[str]
    ) -> Dict[str, Any]:
        """G√©n√®re l'explication structur√©e"""
        
        # üî¥ LOGIQUE CORRIG√âE: Diff√©rencier les cas
        if test_acc >= 0.99:
            # Vrai data leakage - test aussi √† 100%
            confidence_level = "faible"
            summary = (
                f"‚ö†Ô∏è **ALERTE CRITIQUE** : Le mod√®le {model_name} affiche un score test de "
                f"{test_acc*100:.1f}%, ce qui est **tr√®s suspect**. "
                f"Un score de 100% sur le test indique g√©n√©ralement une **fuite de donn√©es** (data leakage). "
                f"Le mod√®le ne doit **PAS** √™tre utilis√© en production sans investigation approfondie."
            )
            metrics_interpretation = (
                "üî¥ ANOMALIE : Score test parfait = fuite de donn√©es probable. "
                "Une feature contient l'information de la cible."
            )
        elif train_acc >= 0.99 and val_acc >= 0.99:
            # Train/val parfaits mais test inf√©rieur = overfitting s√©v√®re
            confidence_level = "faible"
            summary = (
                f"‚ö†Ô∏è **OVERFITTING S√âV√àRE** : Le mod√®le {model_name} atteint 100% sur train/val "
                f"mais seulement **{test_acc*100:.1f}%** sur le test. "
                f"Le mod√®le a **m√©moris√©** les donn√©es d'entra√Ænement sans apprendre √† g√©n√©raliser. "
                f"Une r√©gularisation forte est n√©cessaire."
            )
            metrics_interpretation = (
                f"üî¥ Train/Val: 100% vs Test: {test_acc*100:.1f}% = overfitting massif. "
                "Le mod√®le ne g√©n√©ralise pas du tout."
            )
        elif train_val_gap > 0.15:
            confidence_level = "faible"
            summary = (
                f"Le mod√®le {model_name} montre un **√©cart significatif** de {train_val_gap*100:.1f}% "
                f"entre l'entra√Ænement ({train_acc*100:.1f}%) et la validation ({val_acc*100:.1f}%). "
                f"Cela indique de l'**overfitting** : le mod√®le m√©morise les donn√©es d'entra√Ænement "
                f"sans g√©n√©raliser correctement."
            )
            metrics_interpretation = (
                f"‚ö†Ô∏è √âcart train/val de {train_val_gap*100:.1f}% trop √©lev√©. "
                "Simplifier le mod√®le ou augmenter les donn√©es."
            )
        elif ratio > 0.3:
            confidence_level = "faible"
            summary = (
                f"Le mod√®le {model_name} souffre d'un **ratio features/samples critique** ({ratio:.2f}). "
                f"Avec {n_features} features pour seulement {n_train} √©chantillons, "
                f"le mod√®le risque fortement de surapprendre. R√©duire le nombre de features est essentiel."
            )
            metrics_interpretation = (
                f"üî¥ Ratio {ratio:.2f} > 0.3 = haute dimensionnalit√©. "
                "Les performances sont probablement sur√©valu√©es."
            )
        elif ratio > 0.2:
            confidence_level = "mod√©r√©"
            summary = (
                f"Le mod√®le {model_name} atteint {test_acc*100:.1f}% sur le test. "
                f"Le ratio features/samples ({ratio:.2f}) est √©lev√©, ce qui peut affecter la fiabilit√©. "
                f"Des optimisations sont recommand√©es."
            )
            metrics_interpretation = (
                f"‚ö†Ô∏è Ratio {ratio:.2f} l√©g√®rement √©lev√©. Performances √† v√©rifier avec validation crois√©e."
            )
        elif test_acc < 0.5:
            confidence_level = "faible"
            summary = (
                f"Le mod√®le {model_name} n'atteint que {test_acc*100:.1f}% sur le test, "
                f"ce qui est inf√©rieur au hasard pour une classification binaire. "
                f"Les features actuelles ne sont probablement pas pr√©dictives de la cible."
            )
            metrics_interpretation = "üî¥ Performance insuffisante. Revoir la s√©lection de features."
        else:
            confidence_level = "mod√©r√©" if test_acc < 0.7 else "√©lev√©"
            summary = (
                f"Le mod√®le {model_name} atteint {test_acc*100:.1f}% d'accuracy sur les donn√©es de test. "
                f"{'Performance acceptable mais am√©liorable.' if test_acc < 0.7 else 'Bonne performance g√©n√©rale.'}"
            )
            metrics_interpretation = (
                f"{'Performance correcte.' if test_acc < 0.7 else 'Bon √©quilibre train/val/test.'}"
            )
        
        # Analyse qualit√© donn√©es
        if ratio > 0.3:
            ratio_status = "critique"
            data_interpretation = f"üî¥ Ratio CRITIQUE ({ratio:.2f}). Trop de features pour le nombre d'√©chantillons."
        elif ratio > 0.2:
            ratio_status = "√©lev√©"
            data_interpretation = f"‚ö†Ô∏è Ratio √©lev√© ({ratio:.2f}). Risque d'overfitting."
        elif ratio > 0.1:
            ratio_status = "mod√©r√©"
            data_interpretation = f"Ratio acceptable ({ratio:.2f})."
        else:
            ratio_status = "bon"
            data_interpretation = f"‚úÖ Bon ratio ({ratio:.2f}). Marge suffisante pour l'apprentissage."
        
        # Analyse overfitting
        overfitting_analysis = None
        if is_perfect_score or has_data_leakage:
            overfitting_analysis = {
                "detected": True,
                "severity": "√©lev√©e",
                "causes": [
                    "üî¥ Score parfait de 100% sur tous les ensembles",
                    "Fuite de donn√©es probable (la cible est dans les features)",
                    "Ou variable parfaitement corr√©l√©e avec la cible",
                ],
                "impact": "CRITIQUE : Le mod√®le ne g√©n√©ralisera pas sur de nouvelles donn√©es"
            }
        elif train_val_gap > 0.15:
            overfitting_analysis = {
                "detected": True,
                "severity": "mod√©r√©e",
                "causes": [
                    f"√âcart train/val de {train_val_gap*100:.1f}%",
                    "Mod√®le trop complexe pour les donn√©es",
                    "Possible pr√©sence de variables quasi-constantes"
                ],
                "impact": "Le mod√®le m√©morise plut√¥t qu'il n'apprend"
            }
        elif ratio > 0.2:
            overfitting_analysis = {
                "detected": True,
                "severity": "mod√©r√©e",
                "causes": [
                    f"Ratio features/samples √©lev√© ({ratio:.2f})",
                    "Haute dimensionnalit√©",
                ],
                "impact": "Risque de surapprentissage des patterns sp√©cifiques"
            }
        
        return {
            "summary": summary,
            "model_selected": model_name,
            "problem_type": self._format_problem_type(problem_type),
            "confidence_level": confidence_level,
            "metrics_analysis": {
                "train_accuracy": f"{train_acc*100:.1f}%",
                "validation_accuracy": f"{val_acc*100:.1f}%",
                "test_accuracy": f"{test_acc*100:.1f}%",
                "interpretation": metrics_interpretation
            },
            "data_quality": {
                "features": n_features,
                "samples_train": n_train,
                "samples_test": n_test,
                "ratio": f"{ratio:.2f}",
                "ratio_status": ratio_status,
                "interpretation": data_interpretation
            },
            "overfitting_analysis": overfitting_analysis
        }
    
    def _generate_diagnostic(
        self,
        train_acc: float,
        val_acc: float,
        test_acc: float,
        ratio: float,
        n_train: int,
        is_perfect_score: bool,
        has_data_leakage: bool,
        train_val_gap: float,
        overfitting_detected: bool
    ) -> Dict[str, Any]:
        """G√©n√®re le diagnostic avec score de sant√© R√âALISTE"""
        
        # üî¥ CALCUL CORRIG√â DU SCORE DE SANT√â
        health_score = 100
        
        # D√©tection des cas critiques
        is_perfect_test = test_acc >= 0.99
        is_perfect_train_val = train_acc >= 0.99 and val_acc >= 0.99
        
        # P√©nalit√©s selon gravit√©
        if is_perfect_test:
            # Data leakage confirm√© = tr√®s grave
            health_score -= 60
        elif is_perfect_train_val:
            # Overfitting train/val mais test OK = grave mais moins
            health_score -= 35
        
        # P√©nalit√©s ratio
        if ratio > 0.3:
            health_score -= 25
        elif ratio > 0.2:
            health_score -= 15
        elif ratio > 0.15:
            health_score -= 8
        
        # P√©nalit√©s √©cart train/val
        if train_val_gap > 0.2:
            health_score -= 20
        elif train_val_gap > 0.1:
            health_score -= 10
        
        # P√©nalit√©s performance test
        if test_acc < 0.5:
            health_score -= 25
        elif test_acc < 0.6:
            health_score -= 15
        elif test_acc < 0.7:
            health_score -= 8
        
        # P√©nalit√© overfitting flag
        if overfitting_detected:
            health_score -= 10
        
        # P√©nalit√© donn√©es insuffisantes
        if n_train < 100:
            health_score -= 15
        elif n_train < 300:
            health_score -= 8
        
        # Borner le score
        health_score = max(0, min(100, health_score))
        
        # D√©terminer le statut
        if health_score >= 70:
            health_status = "bon"
            summary = "Le mod√®le est globalement sain avec quelques points d'attention."
        elif health_score >= 50:
            health_status = "mod√©r√©"
            summary = "Le mod√®le n√©cessite des optimisations avant utilisation."
        else:
            health_status = "critique"
            if is_perfect_score:
                summary = "‚ö†Ô∏è CRITIQUE : Score parfait suspect. Investiguer la fuite de donn√©es."
            else:
                summary = "‚ö†Ô∏è CRITIQUE : Le mod√®le pr√©sente des probl√®mes majeurs √† corriger."
        
        # G√©n√©rer les checks
        checks = []
        
        # Check 1: Ratio features/samples
        if ratio > 0.3:
            checks.append({
                "name": "Ratio Features/Samples",
                "status": "error",
                "value": f"{ratio:.2f}",
                "message": f"CRITIQUE : {ratio:.2f} > 0.3. Trop de features!"
            })
        elif ratio > 0.2:
            checks.append({
                "name": "Ratio Features/Samples",
                "status": "warning",
                "value": f"{ratio:.2f}",
                "message": f"√âlev√© : {ratio:.2f} > 0.2. R√©duire les features."
            })
        else:
            checks.append({
                "name": "Ratio Features/Samples",
                "status": "ok",
                "value": f"{ratio:.2f}",
                "message": "Ratio acceptable pour l'apprentissage."
            })
        
        # Check 2: √âcart Train/Validation et overfitting
        is_perfect_test = test_acc >= 0.99
        is_perfect_train_val = train_acc >= 0.99 and val_acc >= 0.99
        
        if is_perfect_test:
            checks.append({
                "name": "Performance Test",
                "status": "error",
                "value": f"{test_acc*100:.1f}%",
                "message": "üî¥ Score test parfait = Fuite de donn√©es probable!"
            })
        elif is_perfect_train_val:
            checks.append({
                "name": "Overfitting Train/Val",
                "status": "error",
                "value": "100% train/val",
                "message": f"üî¥ 100% train/val vs {test_acc*100:.1f}% test = overfitting!"
            })
        elif train_val_gap > 0.15:
            checks.append({
                "name": "√âcart Train/Validation",
                "status": "error",
                "value": f"{train_val_gap*100:.1f}%",
                "message": "Overfitting d√©tect√©. Simplifier le mod√®le."
            })
        elif train_val_gap > 0.08:
            checks.append({
                "name": "√âcart Train/Validation",
                "status": "warning",
                "value": f"{train_val_gap*100:.1f}%",
                "message": "L√©ger surapprentissage possible."
            })
        else:
            checks.append({
                "name": "√âcart Train/Validation",
                "status": "ok",
                "value": f"{train_val_gap*100:.1f}%",
                "message": "Bon √©quilibre train/validation."
            })
        
        # Check 3: Performance Test (seulement si pas d√©j√† couvert par check 2)
        if not is_perfect_test and not is_perfect_train_val:
            if test_acc >= 0.7:
                checks.append({
                    "name": "Performance Test",
                    "status": "ok",
                    "value": f"{test_acc*100:.1f}%",
                    "message": "Bonne performance sur donn√©es non vues."
                })
            elif test_acc >= 0.5:
                checks.append({
                    "name": "Performance Test",
                    "status": "warning",
                    "value": f"{test_acc*100:.1f}%",
                    "message": "Performance modeste. Am√©lioration possible."
                })
            else:
                checks.append({
                    "name": "Performance Test",
                    "status": "error",
                    "value": f"{test_acc*100:.1f}%",
                    "message": "Inf√©rieur au hasard. Features non pr√©dictives."
                })
        
        # Check 4: Taille donn√©es
        if n_train >= 500:
            checks.append({
                "name": "Volume de Donn√©es",
                "status": "ok",
                "value": f"{n_train}",
                "message": "Volume suffisant pour l'apprentissage."
            })
        elif n_train >= 200:
            checks.append({
                "name": "Volume de Donn√©es",
                "status": "warning",
                "value": f"{n_train}",
                "message": "Volume limit√©. Plus de donn√©es recommand√©."
            })
        else:
            checks.append({
                "name": "Volume de Donn√©es",
                "status": "error",
                "value": f"{n_train}",
                "message": "Donn√©es insuffisantes pour un ML fiable."
            })
        
        return {
            "health_score": health_score,
            "health_status": health_status,
            "summary": summary,
            "checks": checks
        }
    
    def _generate_recommendations(
        self,
        is_perfect_score: bool,
        has_data_leakage: bool,
        ratio: float,
        train_val_gap: float,
        test_acc: float,
        train_acc: float,
        val_acc: float,
        n_train: int,
        warnings: List[str]
    ) -> List[Dict[str, Any]]:
        """G√©n√®re les recommandations prioris√©es avec code"""
        
        recommendations = []
        rec_id = 1
        
        # D√©tection locale
        is_perfect_test = test_acc >= 0.99
        is_perfect_train_val = train_acc >= 0.99 and val_acc >= 0.99
        
        # üî¥ PRIORIT√â 1: Score parfait = fuite de donn√©es ou overfitting s√©v√®re
        if is_perfect_test or is_perfect_train_val:
            # Diff√©rencier le message selon le cas
            if is_perfect_test:
                title = "üî¥ Investiguer la fuite de donn√©es"
                description = (
                    f"Un score test de {test_acc*100:.1f}% est presque toujours le signe d'une fuite de donn√©es. "
                    "Une feature contient probablement l'information de la cible."
                )
            else:
                title = "üî¥ Corriger l'overfitting s√©v√®re"
                description = (
                    f"Le mod√®le atteint 100% sur train/val mais seulement {test_acc*100:.1f}% sur test. "
                    "Il m√©morise les donn√©es d'entra√Ænement sans g√©n√©raliser."
                )
            
            recommendations.append({
                "id": rec_id,
                "title": title,
                "priority": "haute",
                "category": "Data Leakage" if is_perfect_test else "Overfitting",
                "description": description,
                "actions": [
                    {
                        "step": 1,
                        "action": "Identifier les features parfaitement corr√©l√©es √† la cible",
                        "code": """# Chercher les corr√©lations parfaites
import pandas as pd

# Calculer corr√©lation avec la cible
correlations = df.corr()[target_column].abs().sort_values(ascending=False)
print("Features les plus corr√©l√©es:")
print(correlations.head(10))

# Features avec corr√©lation > 0.95 = suspects
suspects = correlations[correlations > 0.95].index.tolist()
print(f"\\nüî¥ Features suspectes: {suspects}")"""
                    },
                    {
                        "step": 2,
                        "action": "V√©rifier si une feature est d√©riv√©e de la cible",
                        "code": """# Examiner les features suspectes
for col in suspects:
                            if col != target_column:
        print(f"\\n--- {col} ---")
        print(f"Valeurs uniques: {df[col].nunique()}")
        print(f"Corr√©lation: {correlations[col]:.4f}")
        # V√©rifier si c'est un identifiant ou d√©riv√©
        print(df[[col, target_column]].head(10))"""
                    },
                    {
                        "step": 3,
                        "action": "Supprimer les features probl√©matiques et r√©entra√Æner",
                        "code": """# Exclure les features suspectes
features_clean = [f for f in features if f not in suspects]
X_clean = df[features_clean]

# R√©entra√Æner
model.fit(X_clean, y)
print(f"Score apr√®s nettoyage: {model.score(X_test_clean, y_test)}")"""
                    }
                ],
                "expected_impact": "√âliminer l'overfitting artificiel",
                "effort": "Moyen",
                "timeline": "Imm√©diat"
            })
            rec_id += 1
        
        # PRIORIT√â 2: Ratio √©lev√©
        if ratio > 0.2:
            recommendations.append({
                "id": rec_id,
                "title": "R√©duire le nombre de features",
                "priority": "haute" if ratio > 0.3 else "moyenne",
                "category": "Dimensionnalit√©",
                "description": (
                    f"Le ratio features/samples de {ratio:.2f} est trop √©lev√©. "
                    f"Objectif: ratio < 0.1 pour une g√©n√©ralisation fiable."
                ),
                "actions": [
                    {
                        "step": 1,
                        "action": "Appliquer une s√©lection de features bas√©e sur l'importance",
                        "code": """from sklearn.feature_selection import SelectFromModel
from sklearn.ensemble import RandomForestClassifier

# S√©lection bas√©e sur importance
selector = SelectFromModel(
    RandomForestClassifier(n_estimators=100, random_state=42),
    threshold='median'  # Garde les 50% plus importantes
)
X_selected = selector.fit_transform(X_train, y_train)
print(f"Features: {X_train.shape[1]} ‚Üí {X_selected.shape[1]}")"""
                    },
                    {
                        "step": 2,
                        "action": "Utiliser la variance pour √©liminer les features quasi-constantes",
                        "code": """from sklearn.feature_selection import VarianceThreshold

# Supprimer features avec variance < 0.01
selector = VarianceThreshold(threshold=0.01)
X_filtered = selector.fit_transform(X_train)
kept_features = X_train.columns[selector.get_support()].tolist()
print(f"Features conserv√©es: {len(kept_features)}")"""
                    }
                ],
                "expected_impact": f"R√©duire ratio de {ratio:.2f} √† < 0.1",
                "effort": "Faible",
                "timeline": "1-2 heures"
            })
            rec_id += 1
        
        # PRIORIT√â 3: Overfitting (√©cart train/val)
        if train_val_gap > 0.1 and not is_perfect_score:
            recommendations.append({
                "id": rec_id,
                "title": "Augmenter la r√©gularisation",
                "priority": "haute" if train_val_gap > 0.2 else "moyenne",
                "category": "R√©gularisation",
                "description": (
                    f"L'√©cart de {train_val_gap*100:.1f}% entre train et validation "
                    "indique un surapprentissage."
                ),
                "actions": [
                    {
                        "step": 1,
                        "action": "Pour Decision Tree / Random Forest: limiter la profondeur",
                        "code": """from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier

# Decision Tree r√©gularis√©
dt = DecisionTreeClassifier(
    max_depth=5,           # Limiter profondeur
    min_samples_split=10,  # Min samples pour split
    min_samples_leaf=5,    # Min samples par feuille
    random_state=42
)

# Random Forest r√©gularis√©  
rf = RandomForestClassifier(
    n_estimators=100,
    max_depth=5,
    min_samples_leaf=5,
    random_state=42
)"""
                    },
                    {
                        "step": 2,
                        "action": "Pour Logistic Regression: augmenter la p√©nalit√©",
                        "code": """from sklearn.linear_model import LogisticRegression

# Forte r√©gularisation (C petit = plus r√©gularis√©)
lr = LogisticRegression(
    C=0.01,              # Forte r√©gularisation
    penalty='l2',
    solver='lbfgs',
    max_iter=1000,
    random_state=42
)"""
                    }
                ],
                "expected_impact": f"R√©duire l'√©cart √† < 5%",
                "effort": "Faible",
                "timeline": "30 minutes"
            })
            rec_id += 1
        
        # PRIORIT√â 4: Performance faible
        if test_acc < 0.6 and not is_perfect_score:
            recommendations.append({
                "id": rec_id,
                "title": "Am√©liorer la qualit√© des features",
                "priority": "moyenne",
                "category": "Feature Engineering",
                "description": (
                    f"La performance de {test_acc*100:.1f}% sugg√®re que les features "
                    "actuelles ne capturent pas bien le signal."
                ),
                "actions": [
                    {
                        "step": 1,
                        "action": "Analyser la distribution de la cible",
                        "code": """# V√©rifier le d√©s√©quilibre des classes
print("Distribution de la cible:")
print(y.value_counts(normalize=True))

# Si d√©s√©quilibr√©, utiliser SMOTE
from imblearn.over_sampling import SMOTE
smote = SMOTE(random_state=42)
X_resampled, y_resampled = smote.fit_resample(X_train, y_train)"""
                    },
                    {
                        "step": 2,
                        "action": "Cr√©er des features d'interaction",
                        "code": """from sklearn.preprocessing import PolynomialFeatures

# Cr√©er interactions de degr√© 2
poly = PolynomialFeatures(degree=2, include_bias=False, interaction_only=True)
X_interactions = poly.fit_transform(X_train)
print(f"Features avec interactions: {X_interactions.shape[1]}")"""
                    }
                ],
                "expected_impact": "Am√©liorer accuracy de 10-20%",
                "effort": "Moyen",
                "timeline": "2-4 heures"
            })
            rec_id += 1
        
        # PRIORIT√â 5: Donn√©es insuffisantes
        if n_train < 300:
            recommendations.append({
                "id": rec_id,
                "title": "Utiliser la validation crois√©e",
                "priority": "moyenne",
                "category": "Validation",
                "description": (
                    f"Avec seulement {n_train} √©chantillons, la validation crois√©e "
                    "donne une estimation plus fiable des performances."
                ),
                "actions": [
                    {
                        "step": 1,
                        "action": "Impl√©menter une validation crois√©e stratifi√©e",
                        "code": """from sklearn.model_selection import cross_val_score, StratifiedKFold

# Validation crois√©e 5-fold stratifi√©e
cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
scores = cross_val_score(model, X, y, cv=cv, scoring='accuracy')

print(f"Scores CV: {scores}")
print(f"Moyenne: {scores.mean():.3f} (+/- {scores.std()*2:.3f})"""
                    }
                ],
                "expected_impact": "Estimation fiable des performances",
                "effort": "Faible",
                "timeline": "15 minutes"
            })
            rec_id += 1
        
        # Toujours ajouter une recommandation g√©n√©rale si peu de recs
        if len(recommendations) < 2:
            recommendations.append({
                "id": rec_id,
                "title": "Documenter et versionner le mod√®le",
                "priority": "basse",
                "category": "MLOps",
                "description": "Bonnes pratiques pour la reproductibilit√©.",
                "actions": [
                    {
                        "step": 1,
                        "action": "Sauvegarder le mod√®le avec m√©tadonn√©es",
                        "code": """import joblib
from datetime import datetime

# Sauvegarder avec m√©tadonn√©es
model_info = {
    'model': model,
    'features': feature_names,
    'metrics': {'accuracy': test_acc, 'f1': f1_score},
    'trained_at': datetime.now().isoformat()
}
joblib.dump(model_info, 'model_v1.joblib')"""
                    }
                ],
                "expected_impact": "Reproductibilit√© et tra√ßabilit√©",
                "effort": "Faible",
                "timeline": "30 minutes"
            })
        
        return recommendations
    
    def _generate_tts(
        self,
        model_name: str,
        test_acc: float,
        diagnostic: Dict[str, Any],
        is_perfect_score: bool,
        train_acc: float = 0,
        val_acc: float = 0
    ) -> str:
        """G√©n√®re le texte pour la synth√®se vocale"""
        
        health_status = diagnostic.get('health_status', 'mod√©r√©')
        health_score = diagnostic.get('health_score', 50)
        
        is_perfect_test = test_acc >= 0.99
        is_perfect_train_val = train_acc >= 0.99 and val_acc >= 0.99
        
        if is_perfect_test:
            return (
                f"Attention, alerte critique. Le mod√®le {model_name} affiche un score test parfait "
                f"de {test_acc*100:.0f} pour cent, ce qui est tr√®s suspect. Cela indique probablement une fuite "
                f"de donn√©es. Le score de sant√© est de {health_score} sur 100, statut critique. "
                f"Il est imp√©ratif d'investiguer les features avant toute utilisation."
            )
        elif is_perfect_train_val:
            return (
                f"Attention, overfitting d√©tect√©. Le mod√®le {model_name} atteint 100 pour cent "
                f"sur l'entra√Ænement mais seulement {test_acc*100:.0f} pour cent sur le test. "
                f"Score de sant√©: {health_score} sur 100. Le mod√®le m√©morise les donn√©es "
                f"sans g√©n√©raliser. Une r√©gularisation est n√©cessaire."
            )
        elif health_status == 'critique':
            return (
                f"Le mod√®le {model_name} pr√©sente des probl√®mes critiques. "
                f"Score de sant√©: {health_score} sur 100. "
                f"Des corrections sont n√©cessaires avant utilisation."
            )
        elif health_status == 'mod√©r√©':
            return (
                f"Le mod√®le {model_name} atteint {test_acc*100:.0f} pour cent d'accuracy. "
                f"Score de sant√©: {health_score} sur 100, statut mod√©r√©. "
                f"Des optimisations sont recommand√©es pour am√©liorer la fiabilit√©."
            )
        else:
            return (
                f"Le mod√®le {model_name} est performant avec {test_acc*100:.0f} pour cent d'accuracy. "
                f"Score de sant√©: {health_score} sur 100. Le mod√®le peut √™tre utilis√© avec confiance."
            )
    
    def _format_problem_type(self, problem_type: str) -> str:
        """Formate le type de probl√®me"""
        formats = {
            'binary_classification': 'Classification Binaire',
            'multiclass_classification': 'Classification Multi-classe',
            'regression': 'R√©gression',
            'clustering': 'Clustering'
        }
        return formats.get(problem_type, problem_type)


# Instance singleton
llm_explainer_service = LLMExplainerService()