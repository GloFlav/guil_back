# backend/services/multi_llm_orchestration.py
"""
Service d'orchestration parall√®le multi-LLM
G√®re la g√©n√©ration parall√®le des sections de questionnaire avec OpenAI, Anthropic, Gemini
"""

import logging
import json
import asyncio
from typing import Dict, Any, Optional, Callable, List, Union
from openai import OpenAI
import anthropic
import google.generativeai as genai
from config.settings import settings
from models.survey import Category, Question, ExpectedAnswer, QuestionType, AnswerType, ContextExtraction

logger = logging.getLogger(__name__)

class MultiLLMOrchestrationService:
    """Service pour orchestrer la g√©n√©ration parall√®le avec plusieurs LLM"""
    
    def __init__(self):
        """Initialise les clients LLM"""
        self._init_clients()
    
    def _init_clients(self):
        """Initialise les clients pour chaque LLM"""
        openai_keys = settings.get_openai_keys()
        anthropic_keys = settings.get_anthropic_keys()
        gemini_keys = settings.get_gemini_keys()
        
        self.openai_client = OpenAI(api_key=openai_keys[0]) if openai_keys else None
        self.openai_model = settings.openai_model
        
        self.anthropic_client = anthropic.Anthropic(api_key=anthropic_keys[0]) if anthropic_keys else None
        self.anthropic_model = settings.anthropic_model
        
        if gemini_keys:
            genai.configure(api_key=gemini_keys[0])
        self.gemini_model = settings.gemini_model
        
        logger.info(f"Clients LLM initialis√©s: OpenAI={bool(self.openai_client)}, "
                   f"Anthropic={bool(self.anthropic_client)}, Gemini={bool(gemini_keys)}")
    
    def _get_generation_system_prompt(self) -> str:
        """Retourne le prompt syst√®me pour la g√©n√©ration"""
        return """Tu es un expert en cr√©ation de questionnaires d'enqu√™te professionnels.
        
G√©n√®re les sections du questionnaire bas√©es sur les cat√©gories fournies.

R√àGLES OBLIGATOIRES:
- Chaque question DOIT avoir un ID unique (q1, q2, q3, etc.)
- Utilise les types de questions: single_choice, multiple_choice, text, scale, yes_no, number, date
- Chaque question doit avoir 2-5 r√©ponses possibles
- Inclus une logique conditionnelle avec next_question_id quand pertinent
- Les r√©ponses doivent √™tre d√©taill√©es et professionnelles
- Adapte au contexte fran√ßais/malgache

Format de r√©ponse: JSON uniquement, sans texte suppl√©mentaire."""
    
    def _get_generation_schema(self) -> str:
        """Retourne le sch√©ma JSON pour la g√©n√©ration"""
        return """{
            "categories": [
                {
                    "category_id": "cat1",
                    "category_name": "string",
                    "description": "string",
                    "order": 1,
                    "questions": [
                        {
                            "question_id": "q1",
                            "question_type": "single_choice|multiple_choice|text|scale|yes_no|date|number",
                            "question_text": "string",
                            "is_required": true,
                            "help_text": "string ou null",
                            "predecessor_answer_id": null,
                            "expected_answers": [
                                {
                                    "answer_id": "a1",
                                    "answer_type": "option|text|number|scale|boolean|date",
                                    "answer_text": "string",
                                    "next_question_id": "q2 ou null"
                                }
                            ]
                        }
                    ]
                }
            ]
        }"""
    
    async def generate_category_section(
        self,
        provider: str,
        categories: List[str],
        category_indices: List[int],
        context: Union[Dict[str, Any], ContextExtraction],
        attempt: int = 0
    ) -> Dict[str, Any]:
        """
        G√©n√®re une section de cat√©gories avec un LLM sp√©cifique
        
        Args:
            provider: Fournisseur LLM ('openai', 'anthropic', 'gemini')
            categories: Liste de toutes les cat√©gories
            category_indices: Indices des cat√©gories √† g√©n√©rer par ce provider
            context: Contexte d'extraction (dict ou ContextExtraction)
            attempt: Num√©ro de tentative
        
        Returns:
            Dict avec les cat√©gories g√©n√©r√©es
        """
        try:
            # Convertir context dict en ContextExtraction si n√©cessaire
            if isinstance(context, dict):
                ctx_dict = context
            else:
                ctx_dict = context.dict()
            
            # S√©lectionner les cat√©gories pour ce provider
            assigned_categories = [categories[i] for i in category_indices if i < len(categories)]
            
            logger.info(f"[{provider.upper()}] G√©n√©ration des cat√©gories: {assigned_categories}")
            
            # Construction du prompt
            prompt = f"""
G√©n√®re les questions pour ces cat√©gories:
Objectif: {ctx_dict.get('survey_objective', 'Non sp√©cifi√©')}
Cat√©gories √† g√©n√©rer: {', '.join(assigned_categories)}
Nombre total de questions pour {len(assigned_categories)} cat√©gories: {ctx_dict.get('number_of_questions', 30) // max(len(categories), 1)} questions par cat√©gorie
Audience cible: {ctx_dict.get('target_audience', 'G√©n√©ral')}

Sch√©ma attendu:
{self._get_generation_schema()}

R√©ponds UNIQUEMENT avec un JSON valide."""
            
            # Appel au LLM appropri√©
            if provider == "openai":
                result = await self._generate_openai(prompt)
            elif provider == "anthropic":
                result = await self._generate_anthropic(prompt)
            elif provider == "gemini":
                result = await self._generate_gemini(prompt)
            else:
                return {"success": False, "error": f"Provider inconnu: {provider}"}
            
            if result["success"]:
                logger.info(f"[{provider.upper()}] G√©n√©ration r√©ussie")
                return result
            else:
                logger.warning(f"[{provider.upper()}] Erreur: {result.get('error')}")
                return result
        
        except Exception as e:
            logger.error(f"[{provider.upper()}] Exception: {e}", exc_info=True)
            return {"success": False, "error": str(e)}
    
    async def _generate_openai(self, prompt: str) -> Dict[str, Any]:
        """G√©n√®re avec OpenAI"""
        try:
            if not self.openai_client:
                return {"success": False, "error": "Client OpenAI non disponible"}
            
            response = self.openai_client.chat.completions.create(
                model=self.openai_model,
                messages=[
                    {"role": "system", "content": self._get_generation_system_prompt()},
                    {"role": "user", "content": prompt}
                ],
                max_tokens=2500,
                temperature=0.7,
            )
            
            content = response.choices[0].message.content.strip()
            
            # Nettoyage du JSON
            if content.startswith("```json"):
                content = content[7:]
            if content.endswith("```"):
                content = content[:-3]
            content = content.strip()
            
            data = json.loads(content)
            return {"success": True, "data": data}
        
        except Exception as e:
            logger.error(f"Erreur OpenAI: {e}")
            return {"success": False, "error": str(e)}
    
    async def _generate_anthropic(self, prompt: str) -> Dict[str, Any]:
        """G√©n√®re avec Anthropic Claude"""
        try:
            if not self.anthropic_client:
                return {"success": False, "error": "Client Anthropic non disponible"}
            
            message = self.anthropic_client.messages.create(
                model=self.anthropic_model,
                max_tokens=2500,
                system=self._get_generation_system_prompt(),
                messages=[
                    {"role": "user", "content": prompt}
                ]
            )
            
            content = message.content[0].text.strip()
            
            # Nettoyage du JSON
            if content.startswith("```json"):
                content = content[7:]
            if content.endswith("```"):
                content = content[:-3]
            content = content.strip()
            
            data = json.loads(content)
            return {"success": True, "data": data}
        
        except Exception as e:
            logger.error(f"Erreur Anthropic: {e}")
            return {"success": False, "error": str(e)}
    
    async def _generate_gemini(self, prompt: str) -> Dict[str, Any]:
        """G√©n√®re avec Google Gemini"""
        try:
            if not settings.get_gemini_keys():
                return {"success": False, "error": "Client Gemini non disponible"}
            
            model = genai.GenerativeModel(self.gemini_model)
            
            response = model.generate_content(
                f"{self._get_generation_system_prompt()}\n\n{prompt}",
                generation_config=genai.types.GenerationConfig(
                    max_output_tokens=2500,
                    temperature=0.7
                )
            )
            
            content = response.text.strip()
            
            # Nettoyage du JSON
            if content.startswith("```json"):
                content = content[7:]
            if content.endswith("```"):
                content = content[:-3]
            content = content.strip()
            
            data = json.loads(content)
            return {"success": True, "data": data}
        
        except Exception as e:
            logger.error(f"Erreur Gemini: {e}")
            return {"success": False, "error": str(e)}
    
    async def generate_survey_sections_parallel(
        self,
        context: Union[Dict[str, Any], ContextExtraction],
        progress_callback: Optional[Callable] = None
    ) -> Dict[str, Any]:
        """
        G√©n√®re les sections du questionnaire en parall√®le
        
        Args:
            context: Contexte d'extraction (dict ou ContextExtraction)
            progress_callback: Callback pour la progression
        
        Returns:
            Dict avec toutes les cat√©gories g√©n√©r√©es
        """
        try:
            # Convertir context dict en dict si n√©cessaire
            if isinstance(context, dict):
                ctx_dict = context
            else:
                ctx_dict = context.dict()
            
            categories = ctx_dict.get('categories', []) or [
                "Informations g√©n√©rales",
                "Situation actuelle", 
                "Probl√®mes et d√©fis",
                "Besoins et priorit√©s",
                "Suggestions d'am√©lioration"
            ]
            
            if progress_callback:
                await progress_callback("üöÄ D√©marrage de la g√©n√©ration parall√®le", "starting")
            
            # Distribution des cat√©gories aux providers
            # OpenAI: cat√©gories 0-1, Anthropic: cat√©gories 2-3, Gemini: cat√©gories 4-5
            tasks = [
                self.generate_category_section("openai", categories, [0, 1], ctx_dict),
                self.generate_category_section("anthropic", categories, [2, 3], ctx_dict),
                self.generate_category_section("gemini", categories, [4] if len(categories) > 4 else [], ctx_dict)
            ]
            
            # Ex√©cution parall√®le
            if progress_callback:
                await progress_callback("üîÑ G√©n√©ration OpenAI et Anthropic en parall√®le", "generation")
            
            results = await asyncio.gather(*tasks, return_exceptions=True)
            
            # Fusion des r√©sultats
            all_categories = []
            total_questions = 0
            
            for i, result in enumerate(results):
                if isinstance(result, Exception):
                    logger.error(f"Exception dans la g√©n√©ration {i}: {result}")
                    continue
                
                if isinstance(result, dict) and result.get("success"):
                    categories_data = result.get("data", {}).get("categories", [])
                    all_categories.extend(categories_data)
                    total_questions += sum(len(cat.get("questions", [])) for cat in categories_data)
                else:
                    logger.warning(f"Erreur g√©n√©ration {i}: {result.get('error') if isinstance(result, dict) else str(result)}")
            
            if progress_callback:
                await progress_callback(
                    f"‚úÖ {len(all_categories)} cat√©gories g√©n√©r√©es ({total_questions} questions)",
                    "complete"
                )
            
            return {
                "success": True,
                "categories": all_categories,
                "total_questions": total_questions
            }
        
        except Exception as e:
            logger.error(f"Erreur orchestration: {e}", exc_info=True)
            return {"success": False, "error": str(e)}

# Instance globale
multi_llm_orchestration = MultiLLMOrchestrationService()