import asyncio
import json
import logging
from openai import AsyncOpenAI, APIError, APIConnectionError, RateLimitError
from anthropic import Anthropic
from typing import List, Dict, Any, Tuple, Optional
from models.analysis import Insight

logger = logging.getLogger(__name__)

class MultiLLMInsights:
    """
    Service multi-LLM COMPATIBLE avec l'ancien code.
    - Pas de d√©pendance tenacity
    - Fallback intelligent Claude‚ÜíOpenAI‚ÜíManuel
    - Parsing JSON robuste
    - Retry manuel avec backoff exponentiel
    """

    def __init__(self, settings=None):
        self.settings = settings
        self.max_retries = 2
        self.retry_delay = 2  # secondes, augmente de mani√®re exponentielle

    # =========================================================
    # RETRY MANUEL (Sans tenacity)
    # =========================================================

    async def _call_with_retry(self, func, *args, max_retries=2, **kwargs):
        """Appelle une fonction avec retry manuel et backoff exponentiel."""
        delay = self.retry_delay
        
        for attempt in range(max_retries + 1):
            try:
                result = await func(*args, **kwargs)
                return result
            except (APIError, APIConnectionError, RateLimitError) as e:
                if attempt < max_retries:
                    logger.warning(f"‚ö†Ô∏è Tentative {attempt + 1} √©chou√©e. Retry dans {delay}s...")
                    await asyncio.sleep(delay)
                    delay *= 2  # Backoff exponentiel
                else:
                    logger.error(f"‚ùå Toutes les tentatives √©chou√©es: {e}")
                    return None
            except asyncio.TimeoutError:
                logger.error(f"‚ùå Timeout apr√®s {attempt + 1} tentatives")
                return None
            except Exception as e:
                logger.error(f"‚ùå Erreur inattendue: {e}")
                return None

        return None

    # =========================================================
    # PARSING JSON ROBUSTE
    # =========================================================

    @staticmethod
    def _extract_and_parse_json(text: str) -> Optional[Dict]:
        """Extrait le JSON du texte LLM et le parse."""
        if not text:
            return None

        # Cas 1: JSON pur
        try:
            return json.loads(text)
        except json.JSONDecodeError:
            pass

        # Cas 2: JSON dans le texte (cherche {... } ou [...])
        import re
        
        # Cherche le JSON object
        match = re.search(r'\{[\s\S]*\}', text)
        if match:
            try:
                return json.loads(match.group())
            except json.JSONDecodeError:
                pass

        # Cherche JSON array
        match = re.search(r'\[[\s\S]*\]', text)
        if match:
            try:
                parsed = json.loads(match.group())
                # Si c'est une liste, retourne le premier √©l√©ment ou la liste
                if isinstance(parsed, list) and len(parsed) > 0:
                    return parsed[0] if isinstance(parsed[0], dict) else None
                return parsed
            except json.JSONDecodeError:
                pass

        logger.warning(f"‚ö†Ô∏è Impossible de parser JSON de: {text[:80]}")
        return None

    @staticmethod
    def _validate_insight_structure(obj: Any) -> bool:
        """Valide la structure d'un insight."""
        if not isinstance(obj, dict):
            return False
        
        required_keys = {'title', 'summary', 'recommendation'}
        return all(
            key in obj and 
            isinstance(obj[key], str) and 
            len(obj[key].strip()) > 0
            for key in required_keys
        )

    @staticmethod
    def _create_fallback_insight(error_msg: str = "") -> Dict:
        """Cr√©e un insight de fallback sans LLM."""
        return {
            "title": "‚ö†Ô∏è Analyse Partielle",
            "summary": "Les donn√©es ont √©t√© analys√©es mais les insights IA n'ont pu √™tre g√©n√©r√©s.",
            "recommendation": "V√©rifiez les graphiques et statistiques pour explorer vos donn√©es."
        }

    # =========================================================
    # APPELS LLM
    # =========================================================

    async def _call_openai(self, prompt: str, data: str, task_id: str = "task") -> Optional[Dict]:
        """Appel OpenAI avec gestion d'erreur."""
        try:
            logger.info(f"üìû Appel OpenAI pour {task_id}")
            
            from config.settings import settings
            openai_keys = settings.get_openai_keys()
            
            if not openai_keys:
                logger.warning("‚ö†Ô∏è Aucune cl√© OpenAI disponible")
                return None

            client = AsyncOpenAI(api_key=openai_keys[0])
            
            full_prompt = f"""{prompt}

Donn√©es:
{data}"""

            response = await asyncio.wait_for(
                client.chat.completions.create(
                    model=settings.openai_model,
                    messages=[
                        {
                            "role": "user",
                            "content": full_prompt
                        }
                    ],
                    temperature=0.3,
                    max_tokens=500
                ),
                timeout=30
            )

            response_text = response.choices[0].message.content
            logger.info(f"‚úì R√©ponse OpenAI re√ßue ({len(response_text)} chars)")

            # Parse JSON
            parsed = self._extract_and_parse_json(response_text)
            
            if parsed and self._validate_insight_structure(parsed):
                return parsed

            logger.warning(f"‚ö†Ô∏è Structure invalide: {response_text[:100]}")
            return None

        except (APIError, APIConnectionError, RateLimitError, asyncio.TimeoutError) as e:
            logger.error(f"‚ùå Erreur API OpenAI: {type(e).__name__}")
            return None
        except Exception as e:
            logger.error(f"‚ùå Erreur OpenAI: {e}")
            return None

    async def _call_claude(self, prompt: str, data: str, task_id: str = "task") -> Optional[Dict]:
        """Appel Claude avec gestion d'erreur."""
        try:
            logger.info(f"üìû Appel Claude pour {task_id}")
            
            from config.settings import settings
            anthropic_keys = settings.get_anthropic_keys()
            
            if not anthropic_keys:
                logger.warning("‚ö†Ô∏è Aucune cl√© Anthropic disponible")
                return None

            client = Anthropic(api_key=anthropic_keys[0])

            full_prompt = f"""{prompt}

Donn√©es:
{data}"""

            message = client.messages.create(
                model=settings.anthropic_model,
                max_tokens=500,
                messages=[
                    {
                        "role": "user",
                        "content": full_prompt
                    }
                ],
                timeout=30
            )

            response_text = message.content[0].text
            logger.info(f"‚úì R√©ponse Claude re√ßue ({len(response_text)} chars)")

            # Parse JSON
            parsed = self._extract_and_parse_json(response_text)
            
            if parsed and self._validate_insight_structure(parsed):
                return parsed

            logger.warning(f"‚ö†Ô∏è Structure invalide: {response_text[:100]}")
            return None

        except Exception as e:
            logger.error(f"‚ùå Erreur Claude: {e}")
            return None

    # =========================================================
    # API PRINCIPALE (Compatible avec ancien code)
    # =========================================================

    async def run_parallel_analysis(self, tasks_data: List[Dict[str, Any]]) -> List[Insight]:
        """
        Lance plusieurs t√¢ches d'analyse LLM en parall√®le.
        
        Format des tasks:
        [
            {
                "prompt": "Analyse...",
                "data": "{...json...}",
                "task_id": "optional"  # Optionnel
            }
        ]
        
        Returns:
            List[Insight] : Insights valides g√©n√©r√©s ou fallback
        """
        if not tasks_data:
            logger.warning("‚ö†Ô∏è Aucune t√¢che √† ex√©cuter")
            return []

        logger.info(f"üöÄ Lancement {len(tasks_data)} t√¢ches LLM")

        results = []
        
        for task in tasks_data:
            try:
                prompt = task.get("prompt", "")
                data = task.get("data", "")
                task_id = task.get("task_id", "unknown")

                if not prompt or not data:
                    logger.warning(f"‚ö†Ô∏è T√¢che {task_id}: prompt ou data manquant")
                    results.append(self._create_fallback_insight())
                    continue

                # Strat√©gie: Essayer Claude d'abord, puis OpenAI
                logger.info(f"‚Üí T√¢che {task_id}: Tentative 1 (Claude)")
                insight = await self._call_claude(prompt, data, task_id)

                # Fallback OpenAI
                if not insight:
                    logger.info(f"‚Üí T√¢che {task_id}: Tentative 2 (OpenAI)")
                    insight = await self._call_openai(prompt, data, task_id)

                # Fallback manuel
                if not insight:
                    logger.warning(f"‚ö†Ô∏è T√¢che {task_id}: Fallback manuel")
                    insight = self._create_fallback_insight()

                # Convertir en Insight object si n√©cessaire
                if isinstance(insight, dict):
                    try:
                        results.append(Insight(**insight))
                        logger.info(f"‚úì T√¢che {task_id}: OK")
                    except TypeError:
                        # Si Insight ne peut pas √™tre cr√©√© avec ces args, garder le dict
                        results.append(insight)
                else:
                    results.append(insight)

            except Exception as e:
                logger.error(f"‚ùå Erreur t√¢che {task.get('task_id', '?')}: {e}")
                results.append(self._create_fallback_insight(str(e)))

        logger.info(f"‚úì Analyse parall√®le termin√©e: {len(results)} insights")
        return results

    # =========================================================
    # POUR COMPATIBILIT√â: G√©n√©rateur de t√¢ches
    # =========================================================

    @staticmethod
    def create_task(prompt: str, data: str, task_id: str = "task") -> Dict:
        """Helper pour cr√©er une t√¢che."""
        return {
            "prompt": prompt,
            "data": data,
            "task_id": task_id
        }

# =========================================================
# INSTANCIATION GLOBALE
# =========================================================

multi_llm_insights = None

def init_multi_llm_insights(settings=None):
    """Initialise le service."""
    global multi_llm_insights
    multi_llm_insights = MultiLLMInsights(settings)
    logger.info("‚úì Service Multi-LLM initialis√© (sans tenacity)")
    return multi_llm_insights

# Instanciation par d√©faut si besoin
if multi_llm_insights is None:
    try:
        from config.settings import settings
        multi_llm_insights = MultiLLMInsights(settings)
    except:
        multi_llm_insights = MultiLLMInsights()